{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Segmentation\n",
    "\n",
    "### What to Submit\n",
    "Submit this iPython Notebook--containing all your code for the programming exercises below--on [learning suite](https://learningsuite.byu.edu/).\n",
    "\n",
    "Your notebook file should produce the relevant plots and also provide a short write-up with answers to the questions below.\n",
    "\n",
    "Please also fill in here the time that each part took you:\n",
    "* 1. Setting up PyMaxFlow: 10 min\n",
    "* 2. Getting your first successful segmentation: 6 hrs\n",
    "* 3. Adjusting parameters (e.g. $\\lambda$, $\\sigma$) and so forth, to get good results: 2 hrs\n",
    "* 4. Completing the write-up: 30 min\n",
    "\n",
    "Note that there are two folders within the project.  We have provided some images for you to use in testing your implementation, in the `provided images` folder along with their ground-truth segmentations to compare your results to.  Along with these, we want you to provide 2-4 additional images that you select on which you show your results.  <i>Please use the `provided_images` in that path and place any others in the `user_data` folder, and load all of the images (or user input point location files) via the approapriate relative path.  We will drop your notebook file and your `user_data` folder into our folder (which will have the `provided_images` already) and then run your notebook.</i>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Preparation:\n",
    "So that you can focus on the elements of the cost function ( the link weights), you may use a existing implementation of the actual min-cut algorithm itself.  You set up the graph, but it will take care of finding the minimum cut.\n",
    "\n",
    "For this assignment we will be using a python library called PyMaxFlow.  This library is a python wrapper around the original C++ implementation of the min-cut code from [Vladimir Kolmogorov](http://pub.ist.ac.at/%7Evnk/software.html) (who has co-authored several papers on this subject).    \n",
    "\n",
    "Note: For windows users, you will need the Visual C++ compiler in order for PyMaxFlow to work.  If you already have Visual Studio, this shouldn't be a problem but if you just want the compiler without Visual Studio, you can download [Build Tools For Visual Studio 2017](https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2017). Once you have access to the Visual C++ compiler look at the next paragraph for PyMaxFlow installation.\n",
    "\n",
    "PyMaxFlow requires Cython, which should come standard in your anaconda environment but the command to install that will also be included.  To install PyMaxFlow enter the following commands replacing \"YourEnvironmentName\" with the name of your anaconda environment.\n",
    "~~~\n",
    "conda activate YourEnvironemntName\n",
    "conda install cython\n",
    "pip install pymaxflow\n",
    "~~~\n",
    "Once PyMaxFlow is installed, to understand how to use the library, there is a great [tutorial page](http://pmneila.github.io/PyMaxflow/tutorial.html) that shows how to get started with some simple examples.  Do the \"first example\" and perhaps the \"binary image restoration\" as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 13)\n",
      "Maximum flow: 8\n",
      "Segment of the node 0: 1\n",
      "Segment of the node 1: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import maxflow\n",
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "print(maxflow.__version__)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "###### Example 1 ##########\n",
    "# Create a graph with integer capacities, with 2 non-terminal nodes and 2 non-terminal edges.\n",
    "# The constructor parameters (2, 2) are initial estimations of the number of nodes and the number of non-terminal edges. \n",
    "# These estimations do not need to be correct or even approximate (it is possible to set them to 0), but a good estimation \n",
    "# allows for more efficient memory management.\n",
    "g = maxflow.Graph[int](2, 2)\n",
    "# Add two (non-terminal) nodes. Get the index to the first one.\n",
    "nodes = g.add_nodes(2)\n",
    "# Create the non-terminal edges (forwards and backwards) with the given capacities between nodes 0 and 1.\n",
    "g.add_edge(nodes[0], nodes[1], 1, 2)\n",
    "# Set the capacities of the terminal edges...\n",
    "# ...for the first node\n",
    "g.add_tedge(nodes[0], 2, 5)\n",
    "# ...for the second node\n",
    "g.add_tedge(nodes[1], 9, 4)\n",
    "flow = g.maxflow()\n",
    "print(f\"Maximum flow: {flow}\")\n",
    "# Finally, we want to know the the partition given by the minimum cut:\n",
    "# The method get_segment returns 0 when the given node belongs to the \n",
    "# source partition and 1 when the node belongs to the sink partition.\n",
    "print(f\"Segment of the node 0: {g.get_segment(nodes[0])}\")\n",
    "print(f\"Segment of the node 1: {g.get_segment(nodes[1])}\")\n",
    "\n",
    "######## Binary Image Restoration ##########\n",
    "image_path = cwd + '/user_data/binary_image_restoration.png'\n",
    "img = cv.imread(image_path,cv.IMREAD_GRAYSCALE)\n",
    "# Create the graph.\n",
    "g2 = maxflow.Graph[int]()\n",
    "# Add the nodes. nodeids has the identifiers of the nodes in the grid.\n",
    "# Note that nodeids.shape == img.shape\n",
    "nodeids = g2.add_grid_nodes(img.shape)\n",
    "# Add non-terminal edges with the same capacity.\n",
    "g2.add_grid_edges(nodeids, 50)\n",
    "# Add the terminal edges. The image pixels are the capacities\n",
    "# of the edges from the source node. The inverted image pixels\n",
    "# are the capacities of the edges to the sink node.\n",
    "g2.add_grid_tedges(nodeids, img, 255-img)\n",
    "# Find the maximum flow.\n",
    "g2.maxflow()\n",
    "# Get the segments of the nodes in the grid.\n",
    "# sgm.shape == nodeids.shape\n",
    "sgm = g2.get_grid_segments(nodeids)\n",
    "# The labels should be 1 where sgm is False and 0 otherwise.\n",
    "# The method get_grid_segments returns an array with the same shape than nodeids. \n",
    "# It is almost equivalent to calling get_segment once for each node in nodeids, \n",
    "# but much faster, and preserving the shape of the input. For the i-th cell, \n",
    "# the array stores False if the i-th node belongs to the source segment (i.e., the corresponding pixel has the label 1) \n",
    "# and True if the node belongs to the sink segment (i.e., the corresponding pixel has the label 0). \n",
    "# We now get the labels for each pixel:\n",
    "img2 = np.int_(np.logical_not(sgm))\n",
    "# Show the result.\n",
    "plt.imshow(img2,cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Annotations:\n",
    "Graph cut segmentation is an interactive algorithm requiring the user to provide foreground and background seeds.  Provided is a python file that will open a gui and allow you to annotate the image.  This gui is optional and will require additional packages to be installed into your environment.  To install the packages open a terminal and enter the following commands:\n",
    "~~~\n",
    "conda activate YourEnvironmentName\n",
    "conda install scikit-image pillow\n",
    "~~~\n",
    "You can use the gui in the following way:\n",
    "```python\n",
    "import guiseg\n",
    "fore, back = guiseg.get_fore_back(image)\n",
    "image[fore]  # the foreground seeds\n",
    "image[back]  # the background seeds\n",
    "```\n",
    "\n",
    "For the `guiseg` routine to run, I also found it necssary to install PIL ImageTk (For me, it was `sudo apt install python3-pil.imagetk` but it will be different for Conda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import guiseg\n",
    "import cv2\n",
    "#imageBGR = cv2.imread('provided_images/simplecircle.png');\n",
    "imageBGR = cv2.imread('provided_images/banana.png');\n",
    "pic = imageBGR[:,:,::-1]; # I reverse the BGR from OpenCV to BGR\n",
    "\n",
    "# When the GUI pops up, you can pick either the \"Foreground\" or \"Background\" buttons to\n",
    "# select pixels to be respective seeds.  Once you're finished, click \"Return\"\n",
    "fore, back = guiseg.get_fore_back(pic)\n",
    "print(pic[fore][:5])  # foreground seeds (RGB values for all pixels drawn on, but only showing 5)\n",
    "print(pic[back][:5])  # background seeds (RGB values for all pixels drawn on, but only showing 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Graph Cut:\n",
    "Your code should read in an image and a set of seed pixels and use graph-cut segmentation to segment the image.\n",
    "\n",
    "You will need to calculate the costs for the t-links (region terms) and the n-links (boundary terms). See the book, the notes/slides, or published papers in this area for ideas of how to define these.  Remember that the t-link weights to a particular terminal (foreground or background) should be large if that pixel looks a lot like the respective foreground/background seeds. The n-link weights should be large if the two neighboring pixels are similar.\n",
    "\n",
    "Here is [the original paper on graph-cut segmentation](http://www.csd.uwo.ca/~yuri/Papers/iccv01.pdf), which might help with some ideas, but you should look at the literature to see what other costs functions / link weights others have used.\n",
    "\n",
    "Once the graph is built, use the min-cut algorithm to partition the graph into nodes connected to the foreground node or to the background node, then use these as the resulting labels for the segmentation. Display this result graphically in some fashion overlaid on the input image.  It is best to start with simple images whose foreground and background colors are pretty different and for ones where the edges are pretty clear.  Graph-cut segmentation struggles sometimes with long, thin structures, so you should avoid these types of images early on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 3)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15368/951374348.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0msegmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtri_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_segmentation_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtri_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15368/951374348.py\u001b[0m in \u001b[0;36mget_segmentation_images\u001b[0;34m(image, sigma)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# add terminal edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mforeground_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguiseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fore_back\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mkde_foreground\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelDensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gaussian'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbandwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mforeground_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mkde_background\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelDensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gaussian'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbandwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbackground_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/vision/lib/python3.8/site-packages/sklearn/neighbors/_kde.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0malgorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_choose_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/vision/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/vision/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    806\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 3)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "import copy\n",
    "\n",
    "# Import Image\n",
    "# image_path = cwd + '/provided_images/simplecircle.png'\n",
    "image_path = cwd + '/provided_images/penguin.png'\n",
    "image = cv.imread(image_path)\n",
    "\n",
    "def get_x_pixel_similarity_probability(image,sigma):\n",
    "    height = int(np.shape(image)[0])\n",
    "    width = int(np.shape(image)[1])\n",
    "    padded_image = np.concatenate((image,np.zeros((height,1,3),dtype=int)+125),1)\n",
    "    left_image = padded_image[0:height,0:width,:]\n",
    "    right_image = padded_image[0:height,1:width+1,:]\n",
    "    x_differences = left_image - right_image\n",
    "    norm = np.linalg.norm(x_differences,ord=2,axis=2)\n",
    "    return np.exp(-(norm**2)/(2*sigma**2))\n",
    "\n",
    "def get_y_pixel_similarity_probability(image,sigma):\n",
    "    height = int(np.shape(image)[0])\n",
    "    width = int(np.shape(image)[1])\n",
    "    padded_image = np.concatenate((image,np.zeros((1,width,3),dtype=int)+125),0)\n",
    "    # padded_image = np.concatenate((image,np.zeros((height,1,3),dtype=int)+125),1)\n",
    "    top_image = padded_image[0:height,0:width,:]\n",
    "    bottom_image = padded_image[1:height+1,0:width,:]\n",
    "    y_differences = top_image - bottom_image\n",
    "    norm = np.linalg.norm(y_differences,ord=2,axis=2)\n",
    "    return np.exp(-(norm**2)/(2*sigma**2))\n",
    "\n",
    "def get_segmentation_images(image,sigma):\n",
    "    number_of_nonterminal_nodes = np.shape(image)[0]*np.shape(image)[1]\n",
    "    number_of_terminal_nodes = 2\n",
    "    graph = maxflow.Graph[int](number_of_nonterminal_nodes, number_of_terminal_nodes)\n",
    "    # add nodes\n",
    "    nodeids = graph.add_grid_nodes((image.shape[0],image.shape[1]))\n",
    "    # add terminal edges\n",
    "    foreground_data, background_data = guiseg.get_fore_back(image)\n",
    "    kde_foreground = KernelDensity(kernel = 'gaussian' , bandwidth = 0.4).fit(image[foreground_data])\n",
    "    kde_background = KernelDensity(kernel = 'gaussian' , bandwidth = 0.4).fit(image[background_data])\n",
    "    height = np.shape(image)[0]\n",
    "    width = np.shape(image)[1]\n",
    "    vector_pixels = np.reshape(image,(height*width,3))\n",
    "    foreground_score = np.abs(np.reshape(kde_foreground.score_samples(vector_pixels),(height,width)))\n",
    "    background_score = np.abs(np.reshape(kde_background.score_samples(vector_pixels),(height,width)))\n",
    "    max_likelihood = np.amax([np.amax(foreground_score), np.amax(background_score)])\n",
    "    scale = 255\n",
    "    foreground_score = foreground_score*scale/max_likelihood\n",
    "    background_score = background_score*scale/max_likelihood\n",
    "    graph.add_grid_tedges(nodeids, foreground_score, background_score)\\\n",
    "    # add non terminal edges\n",
    "    x_similarity= get_x_pixel_similarity_probability(image,sigma)\n",
    "    y_similarity = get_y_pixel_similarity_probability(image,sigma)\n",
    "    horizontal_structure = np.array([[0, 0, 0],\n",
    "                                    [0, 0, 1],\n",
    "                                    [0, 0, 0]])\n",
    "    vertical_structure = np.array([[0, 0, 0],\n",
    "                                   [0, 0, 0],\n",
    "                                   [0, 1, 0]])\n",
    "    llambda = np.amax([np.amax(foreground_score) ,np.amax(background_score)])\n",
    "    graph.add_grid_edges(nodeids,weights = x_similarity*llambda, structure = horizontal_structure, symmetric = True)\n",
    "    graph.add_grid_edges(nodeids,weights = y_similarity*llambda, structure = vertical_structure, symmetric = True)\n",
    "    graph.maxflow()\n",
    "    segmented = graph.get_grid_segments(nodeids)\n",
    "    tri_map = (copy.deepcopy(image)/5+77).astype(np.uint8)\n",
    "    tri_map[foreground_data] = 255\n",
    "    tri_map[background_data] = 0\n",
    "    return segmented, tri_map\n",
    "\n",
    "sigma = 10\n",
    "segmented, tri_map = get_segmentation_images(image,sigma)\n",
    "plt.subplots(1,4,1)\n",
    "plt.imshow(tri_map)\n",
    "\n",
    "plt.subplots(1,4,2)\n",
    "plt.imshow(segmented)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate results Here (again, add additional cells to your heart's content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Grading / Rubric\n",
    "Points for this assigment will be assigned as follows (100 points total):\n",
    "* [20 pts] Code that correctly generates the graph network structure (nodes, n-links, t-links).\n",
    "* [10 pts] Code that produces the boundary term $B(p,q)$ used for n-links.\n",
    "* [10 pts] Code that produces the region term of the cost $R(p,A)$ used for the t-links.  Remember that you have t-links per pixel, one with cost determined by matching $p$ with the foreground appearance distribution, the other determined relative to the background distribution.  You may use the [sk-learn implementation](https://scikit-learn.org/stable/modules/density.html#kernel-density-estimation) of Kernel Density Estimation.  However you will receive 10 extra points if you implement it yourself.\n",
    "* [20 pts] Implementing the graph-cut with `pymaxflow` and finding the optimal solution for the input graph.\n",
    "* [10 pts] Displaying Results in the following format (for each input image you'll show the following 3-4 result images):\n",
    "   1. Original Image.\n",
    "   2. Tri-map of what was selected by the user (white for foreground, black for background, gray for unknown).  This can be overlaid on top of a faint copy of the image for context if desired.\n",
    "   3. Final segmentation.  Again you can overlay it on a faint copy of the original for context.\n",
    "   4. On the <i>provided images</i> please show a comparison of your resulting segmentaiton with the ground truth.\n",
    "* [20 pts] Good (certainly not perfect, some of them are challenging, but decent/reasonable) results on the 4 provided images (banana, llama, penguin, teddy).  Each image will receive up to 5 points.\n",
    "* [10 pts] Demonstrating your algorithm on 2-4 additional images.  At least one of the images should be somewhat easy, one should be somewhat challenging -- expalin why you think they're respectively easy/challenging.\n",
    "\n",
    "\n",
    "## Write-up:\n",
    "Provide an explanation for the following items:\n",
    "* Describe how you determinied/computed the n-link and t-link weights.\n",
    "* What kinds of image does graph cut segmentation work well for? What kinds of images do you find it struggles with?\n",
    "* What did you learn from the project?\n",
    "* What if any suggestions do you have for improving it (for future students)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">WRITE-UP HERE</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
